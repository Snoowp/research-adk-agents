Okay, here are a couple of Mermaid diagrams to visualize the system:
File Responsibility & Interaction Diagram: This diagram shows the main Python files and their primary roles or interactions.
Agent Workflow Diagram (Flowchart): This diagram details the step-by-step process flow within the LangGraph agent.
1. File Responsibility & Interaction Diagram
This diagram outlines which file is primarily responsible for what aspect of the backend system.
graph TD
    A_AppPy["app.py <br> FastAPI App, Frontend Serving"]
    B_GraphPy["graph.py <br> Agent Logic, Graph Definition, Node Functions"]
    C_ConfigPy["configuration.py <br> Agent Settings, Model Names"]
    D_PromptsPy["prompts.py <br> LLM Instruction Templates"]
    E_StatePy["state.py <br> Graph State Definitions"]
    F_ToolsSchemasPy["tools_and_schemas.py <br> Pydantic Schemas for LLM Output"]
    G_UtilsPy["utils.py <br> Helper Functions e.g. Citation, URL parsing"]
    H_Frontend["Frontend User Interface <br> Served by app.py"]
    I_LangGraphLib["LangGraph Library"]
    J_GeminiAPI["Google Gemini API"]
    K_GoogleSearchTool["Google Search Tool <br> via Gemini or genai_client"]

    A_AppPy -->|"Serves"| H_Frontend
    A_AppPy -->|"Exposes API for"| B_GraphPy

    B_GraphPy -->|"Uses"| I_LangGraphLib
    B_GraphPy -->|"Defines Nodes using"| J_GeminiAPI
    B_GraphPy -->|"Nodes use"| K_GoogleSearchTool
    B_GraphPy -->|"Reads Config from"| C_ConfigPy
    B_GraphPy -->|"Uses Prompts from"| D_PromptsPy
    B_GraphPy -->|"Manages State via"| E_StatePy
    B_GraphPy -->|"Nodes use Schemas from"| F_ToolsSchemasPy
    B_GraphPy -->|"Nodes use Utils from"| G_UtilsPy

    J_GeminiAPI <--> K_GoogleSearchTool
Use code with caution.
Mermaid
Explanation of File Responsibility Diagram:
app.py: The entry point. It sets up the FastAPI web server, serves the frontend, and would host the API endpoints that trigger the agent.
graph.py: The core of the agent. It defines the LangGraph state machine, its nodes (individual processing steps), and the edges (transitions between steps). It orchestrates calls to Gemini and uses helper modules.
configuration.py: Holds the settings for the agent, like which LLM models to use for different tasks and operational parameters (e.g., max research loops).
prompts.py: Contains all the detailed instructional prompts fed to the Large Language Models (LLMs) at various stages.
state.py: Defines the structure of the data (state) that flows through the LangGraph agent.
tools_and_schemas.py: Provides Pydantic models to enforce a specific structure for the outputs from LLMs, making them easier to parse and use.
utils.py: A collection of helper functions for tasks like formatting text, processing URLs, and handling citations.
2. Agent Workflow Diagram (Flowchart)
This diagram illustrates the sequence of operations performed by the agent defined in graph.py.
flowchart TD
    StartRequest((User Request / Input)) --> Node_GenerateQuery
    
    subgraph AgentGraph [Pro Search Agent - graph.py]
        Node_GenerateQuery["<b>generate_query</b><br>LLM: Generate search queries<br>Output: List of queries"]
        Node_GenerateQuery --> Edge_ContinueToWebResearch{Send to Web Research}
        
        Edge_ContinueToWebResearch -- Query 1 --> Node_WebResearch1["<b>web_research</b> (Branch 1)<br>Search Query 1<br>LLM+Tool: Execute search, get results & citations"]
        Edge_ContinueToWebResearch -- Query N... --> Node_WebResearchN["<b>web_research</b> (Branch N)<br>Search Query N<br>LLM+Tool: Execute search, get results & citations"]
        
        Node_WebResearch1 --> Node_Reflection
        Node_WebResearchN --> Node_Reflection
        
        Node_Reflection["<b>reflection</b><br>Aggregate results<br>LLM: Analyze summary, identify gaps, suggest follow-up queries"]
        Node_Reflection --> Cond_EvaluateResearch{"<b>evaluate_research</b><br>Sufficient info OR Max loops reached?"}
        
        Cond_EvaluateResearch -- No --> Edge_FollowUpToWebResearch{Send Follow-up Queries}
        Edge_FollowUpToWebResearch -- Follow-up Query 1 --> Node_WebResearchFollowUp1["<b>web_research</b> (Follow-up Branch 1)<br>Search Follow-up Query 1<br>LLM+Tool: Execute search"]
        Edge_FollowUpToWebResearch -- Follow-up Query M... --> Node_WebResearchFollowUpM["<b>web_research</b> (Follow-up Branch M)<br>Search Follow-up Query M<br>LLM+Tool: Execute search"]
        Node_WebResearchFollowUp1 --> Node_Reflection
        Node_WebResearchFollowUpM --> Node_Reflection
        
        Cond_EvaluateResearch -- Yes --> Node_FinalizeAnswer["<b>finalize_answer</b><br>LLM: Generate final answer with all sources"]
        Node_FinalizeAnswer --> EndResponse((Final Answer / Output))
    end

    style Node_GenerateQuery fill:#f9f,stroke:#333,stroke-width:2px
    style Node_WebResearch1 fill:#ccf,stroke:#333,stroke-width:2px
    style Node_WebResearchN fill:#ccf,stroke:#333,stroke-width:2px
    style Node_Reflection fill:#f9f,stroke:#333,stroke-width:2px
    style Cond_EvaluateResearch fill:#lightgrey,stroke:#333,stroke-width:2px
    style Node_WebResearchFollowUp1 fill:#ccf,stroke:#333,stroke-width:2px
    style Node_WebResearchFollowUpM fill:#ccf,stroke:#333,stroke-width:2px
    style Node_FinalizeAnswer fill:#f9f,stroke:#333,stroke-width:2px
    style AgentGraph fill:#eee,stroke:#333,stroke-width:2px
Use code with caution.
Mermaid
Explanation of Agent Workflow Diagram:
User Request / Input: The process starts with a question or research topic from the user.
generate_query (Node in graph.py):
The first LLM call happens here to break down the user's request into specific, actionable search queries.
Send to Web Research (Conditional Edge in graph.py):
The continue_to_web_research function dispatches each generated query to a separate, parallel web_research branch.
web_research (Parallel Nodes in graph.py):
For each query, this node uses an LLM (with the Google Search tool) to find information on the web.
It processes results, extracts citations, and shortens URLs.
reflection (Node in graph.py):
All results from the parallel web_research branches are aggregated here.
An LLM analyzes the combined summary to see if the original question is answered or if there are knowledge gaps. It may generate follow-up queries.
evaluate_research (Conditional Edge in graph.py):
This logic decides the next step:
No (More Research Needed): If gaps exist and max_research_loops isn't hit, the follow-up queries are sent to new parallel web_research branches. The process then loops back to reflection.
Yes (Sufficient Info / Max Loops): If the information is deemed sufficient or the loop limit is reached, the process moves to finalize the answer.
finalize_answer (Node in graph.py):
An LLM synthesizes all gathered information, summaries, and the original user query into a comprehensive final answer.
It ensures citations (with original URLs) are correctly included.
Final Answer / Output: The generated answer is returned to the user.
These diagrams should provide a good overview of the system's architecture and its operational flow.